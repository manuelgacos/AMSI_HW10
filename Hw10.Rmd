---
title: "Hw10"
author: "Manuel Alejandro Garcia Acosta"
date: "11/20/2019"
output: pdf_document
---

For this exercise I'll use the 'MASS' package.
```{r}
library(MASS)
setwd('/home/noble_mannu/Documents/PhD/First/STAT_2131_Applied_Statistical_Methods_I/HW10')
```

# Exercise 3

First I read the data from 'Fat.txt'. After that, I construct the data frames for the training data and the test data.

```{r}
# Read the data
Data <- read.table('Fat.txt', header = TRUE)

# Creating the training data frame by removing the tenths observations
tenths <- seq(from = 10, to = 252, by =10)
Data_1 <- Data[-tenths, ]

# Create the test data frame only with the tenths observations
Data_2 <- Data[tenths, ]
```

In part (i) and (ii) I ran both linear regression and ridge regression for the training data, where 'siri' is the response and all other variables are predictors.

## (i) A simple linear model

For this part I just fitted a linear model.

```{r}
# Create the linear model
linMod <- lm(siri ~ ., data = Data_1)
# Display the summary of the linear model
summary(linMod)
```

I also created the design matrix separately.

```{r}
# Create X matrix including intercept for the training data
intercept <- rep(1, length(Data_1$siri))
X <- cbind(intercept, data.matrix(subset(Data_1, select = -c(siri))))
Y <- Data_1$siri
```

## (ii) Ridge regression

Here I fitted a model using ridge regression. The tuning parameter $\lambda$ was chosen using Generalized Cross Validation (GCV).

Choosing the right vector of possible values for $\lambda$ was a little tricky. However, after some attempts I came up with this one that gets the job done.

```{r}
# Create vector of lambdas
lambda_seq <- 10^seq(-1, -4, by = -.05)
```

I used the vector defined above to run ridge regression.

```{r}
# Run ridge regression
fit.ridge <- lm.ridge(siri~ ., data=Data_1, lambda = lambda_seq)
# Save the fitted coefficients for each value of lambda on a matrix
all.coefficients <- data.matrix(coef(fit.ridge))
```

After running the model I needed to choose the best $\lambda$. Such $\lambda$ will minimize the GCV. Here is the plot of GCV as a function of $\lambda$, this gives us a good idea of which value of $\lambda$ to pick.

```{r}
# Here I plot GCV as a function of lambda
plot( lambda_seq, fit.ridge$GCV, xlab = 'Lambda', ylab = 'GCV',
      main = 'GCV as a function of lambda')
```

Since the object 'fit.ridge\$GCV' contains the GCV results for each value of lambda, it was enough to look for the smallest GCV and from there retrieve the value of $\lambda$.

```{r}
# Choosing the best lambda based on GCV
index <- which.min(fit.ridge$GCV)
# Which GCV was the minimum
min_gcv <- fit.ridge$GCV[index]
# Which lambda was the best
best_lambda <- as.numeric(names(fit.ridge$GCV[index]))

best_lambda
```

Now we know that the best value for lambda was $\lambda = 0.04466836$.

## (a) Training error

In this part I computed the training error for both models (linear and ridge). 

### Linear model

First, for the linear model we have the following.

```{r}
# Compute the training error for the linear model
y.lin <- Data_1$siri - linMod$fitted.values
n <- length(Data_1$siri)
err.lin <- 1/n * y.lin%*%y.lin
# Printing the training error for the linear model
err.lin
```

The training error for the linear model was $\overline{err}_{(lin)} = 2.232976$.

### Ridge regression model

Second, I computed the training error for the ridge regression model.

```{r}
# Compute the fitted values for the best lambda (ridge regression)
Y.hat.lambda <- as.vector(X%*%all.coefficients[index,])
# Compute the training error for the ridge regression model
y.ridge <- Data_1$siri - Y.hat.lambda
err.ridge <- 1/n * y.ridge%*%y.ridge
# Printing the training error for the ridge model
err.ridge
```

The training error for the ridge model was $\overline{err}_{(ridge)} = 2.233271$. Therefore the training error for the linear model was smaller than the training error for the ridge model.

```{r}
# We get that the error from the linear model is smaller than the error using ridge
err.lin < err.ridge
```

The training error is a poor judge of how well the model will predict future data since as we proved in exercise 1 the training error usually underestimates the test (or prediction) error. Moreover, if we try to make the training error in a specific model really small then more often than not our model will be almost useless since the test error when we are predicting values will be high. It's kind of overfitting our training model and then finding out that it doesn't work for prediction.

## (b) Test error

In this last section I use the models from part (i) and (ii) to predict the held out data -that is, the data in 'Data_2'-. The metric (loss function) used for this purpose was

$$LF = \frac{1}{n} \sum_{i = 1}^{25} \left[ y_{i} - \hat{f}(x_{i}) \right]^{2}$$

Where $i \in \{1,...,25 \}$ because our test data consist of 25 observations.

Next, I define the design matrix and the response I'll use for computing the loss function.

```{r}
# Create X matrix including intercept for the test data
intercept.test <- rep(1, length(Data_2$siri))
X.test <- cbind(intercept.test, data.matrix(subset(Data_2, select = -c(siri))))
Y.test <- Data_2$siri
# Define n = # of observations. I use the same for both models
n.test <- length(Data_2$siri)
```

### Linear model

In this section I compute the loss function for the linear regression model.

```{r}
# Using the linear model to predict the held out data
lin.coef <- coef(linMod)
fitted.Y.linear <- as.vector(X.test%*%lin.coef)
y.lin.test <- Data_2$siri - fitted.Y.linear
loss.lin <- 1/n.test * y.lin.test%*%y.lin.test
loss.lin
```

Here we have that the loss function for the linear model is $LF_{(lin)} = 1.280357$.

### Ridge regression model

In this section I compute the loss function for the ridge regression model.

```{r}
# Using the ridge model to predict the held out data
fitted.Y.ridge <- as.vector(X.test%*%all.coefficients[index,])
y.ridge.test <- Data_2$siri - fitted.Y.ridge
loss.ridge <- 1/n.test * y.ridge.test%*%y.ridge.test
loss.ridge
```

Here we have that the loss function for the ridge model is $LF_{(ridge)} = 1.272906$. Finally, we conclude finally that the ridge model performed better than the linear regression model at predicting the test data.

```{r}
# Now ridge does a better job at predicting the data
loss.ridge < loss.lin
```
